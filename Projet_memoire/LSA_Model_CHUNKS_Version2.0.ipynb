{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Primary Address Id</td>\n",
       "      <td>Adresse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Primary Ship To Address Id</td>\n",
       "      <td>Age de l'assuré</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Comment Address</td>\n",
       "      <td>Age de permis du conducteur principal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Short Comment</td>\n",
       "      <td>Age du conducteur principal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CO - Age de l'assuré</td>\n",
       "      <td>Age du véhicule assuré</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Name                            Description\n",
       "0          Primary Address Id                                Adresse\n",
       "1  Primary Ship To Address Id                        Age de l'assuré\n",
       "2             Comment Address  Age de permis du conducteur principal\n",
       "3               Short Comment            Age du conducteur principal\n",
       "4        CO - Age de l'assuré                 Age du véhicule assuré"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(r\"C:\\Users\\bouhl\\input_file.xlsx\")\n",
    "#df=df.rename(columns = {'is linked to [Local Data Element] > Name':'Donnee_metier'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1=df.columns[0]\n",
    "col2=df.columns[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1029\n"
     ]
    }
   ],
   "source": [
    "proxy1=[\"N° Personne Sogessur\",\"Numéro de personne\",\"N° de personne,SI - N° Personne\",\"SI - N° Personne Sogessur\",\"N° Personne\"]\n",
    "for i in proxy1:\n",
    "    df[col1] =df[col1].str.replace(i,\"Numéro de client assuré\")\n",
    "    \n",
    "proxy2=[\"CO - Coefficient Commercial\",\"CO - Coefficient Commercial (Réduction Salariés)\"]\n",
    "for i in proxy2:\n",
    "    df[col1] =df[col1].str.replace(i,\"Ancienne réduction salariés (RED)\")\n",
    "    \n",
    "proxy3=[\"SI - Date Saisie Montant\",\"SI - Montant Provisions Restantes\",\"Numéro de Police SOGESSUR\",\"N° Devis\",\"ALD - N° Police SOG\",\"SI - Amount Incurred\",\"CO - Acceptation FID O/N\",\"SI - Montant Total Paiements\",\"CO - CSP de l'assuré principal\",\"SI - City of Accident\"]\n",
    "ch=[\"Date mouvement technique sinistre\",\"Montant de la provision cédée restante\",\"Numéro de contrat\",\"Numéro de contrat\",\"Numéro de contrat\",\"Provision brute à la garantie du sinistre\",\"Coefficient de fidélisation auto\",\"Règlement net de recours à la garantie du sinistre\",\"Catégorie socio-professionnelle de l'assuré\",\"Ville du sinistre MRH\"]\n",
    "for i,j in zip(proxy3,ch):\n",
    "    df['Description'] =df['Description'].str.replace(i,j)\n",
    "\n",
    "proxy4=[\"CO - Commercial Premium Amount\",\"CO - Gross Premium\",\"CO - Montat Prime HT\"]\n",
    "for i in proxy4:\n",
    "    df['Description'] =df['Description'].str.replace(i,\"Montant de la prime commerciale d'assurance hors taxes en devise de reporting\")\n",
    "print(len(df[col1]))\n",
    "\n",
    "df=df.apply(lambda x: x.astype(str).str.lower())       \n",
    "df[col1] =df[col1].str.replace('n°client','numero client')\n",
    "df[col1] =df[col1].str.replace('n°','numero')\n",
    "df[col1]=df[col1].str.replace(\"d'\", '')\n",
    "#df[col1]=df[col1].str.replace('\\d+', '')\n",
    "df[col1] =df[col1].str.replace('[^\\w\\s]',' ')\n",
    "\n",
    "df[col2] = df[col2][(~df[col2].duplicated()) | df[col2].isna()] #drop dup de col2 ignorant les Nan value \n",
    "df[col2] =df[col2].str.replace('n°','numero')\n",
    "df[col2]=df[col2].str.replace(\"d'\", '')\n",
    "#df[col2]=df[col2].str.replace('\\d+', '')\n",
    "df[col2] =df[col2].str.replace('[^\\w\\s]',' ')    \n",
    "\n",
    "l1=['number','region','contract','name','customer','guarantee','address',\"claim\",\"closed\",\"opened\",\"status\",\"HT\",\"amendment\",\"somme\",\"crmr\",\"csdp\",\"sdp\",\"product\"]\n",
    "l2=['numero','pays','contrat','nom','client','garantie','adresse',\"sinistre\",\"cloture\",\"ouverture\",\"etat\",\"Hors Taxes\",\"avenant\",\"montant\",\"crm recalculé\",\"coefficient de surveillance du portefeuille\",\"surveillance du portefeuille\",\"produit\"]\n",
    "for i,j in zip(l1,l2):\n",
    "    df[col1] =df[col1].str.replace(i,j)\n",
    "    \n",
    "#df to list\n",
    "example_des=df.reset_index()[col1].values.tolist()\n",
    "example_m=df.reset_index()[col2].values.tolist()  \n",
    "\n",
    "example_m = list(dict.fromkeys(example_m)) #effacer les doublons \n",
    "\n",
    "example_m = [i for i in example_m if str(i) != 'nan']\n",
    "example_des = [i for i in example_des if str(i) != 'nan']\n",
    "example_des=[i.lstrip() for i in example_des]\n",
    "#example_m = list(dict.fromkeys(example_m)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1029"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example_des)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHUNKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1160\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "def untokenize(words):\n",
    "    text = ' '.join(words) \n",
    "    return text.strip()\n",
    "\n",
    "def accent_remove(s):\n",
    "\n",
    "    text = ''.join((c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')) \n",
    "    return text\n",
    "\n",
    "path=r'C:\\Users\\bouhl'\n",
    "\n",
    "with open(path+\"\\outputt_file1.csv\", mode='w',newline='') as file:\n",
    "    writer = csv.writer(file, delimiter=';', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writerow([\"Data1\",\"Data2\",\"Correlation score\"])\n",
    "    \n",
    "chunk_size=3000\n",
    "chunks = [example_des[x:x+chunk_size] for x in range(0, len(example_des),chunk_size)]\n",
    "\n",
    "for i,j in zip(chunks,range(len(chunks))):\n",
    "    example=example_m+i\n",
    "    \n",
    "    print(len(example))\n",
    "    \n",
    "    df = pd.DataFrame({'example':example})\n",
    "    df['example']= [word_tokenize(entry) for entry in df['example']]\n",
    "    stopWords = set(stopwords.words('French'))\n",
    "    stopWords_ang = set(stopwords.words('English'))\n",
    "    l=[\"-\",\"d\",\"co\",\"si\"]\n",
    "    df['example']=df['example'].apply(lambda x: [item for item in x if item not in stopWords])\n",
    "    df['example']=df['example'].apply(lambda x: [item for item in x if item not in stopWords_ang])\n",
    "    df['example']=df['example'].apply(lambda x: [item for item in x if item not in l])\n",
    "    #stemming_frensh\n",
    "\n",
    "\n",
    "    stemmer = FrenchStemmer()\n",
    "    stem_list=[]\n",
    "    for i in df['example']:\n",
    "        stem_list.append ([stemmer.stem(word) for word in i])\n",
    "\n",
    "    df['example']=[untokenize(entry) for entry in stem_list]\n",
    "    df['example']=[accent_remove(entry) for entry in df['example']]\n",
    "    df['example'] =df['example'].str.replace(' +', ' ')\n",
    "    example=df.values.tolist()\n",
    "    flat_list = lambda l: [item for sublist in l for item in sublist]\n",
    "    example=[i.lstrip() for i in df['example']]\n",
    "    vectorizer = TfidfVectorizer(use_idf=True,smooth_idf=True,sublinear_tf=True)\n",
    "    dtm = vectorizer.fit_transform(example)\n",
    "    from scipy.sparse import csr_matrix\n",
    "    X_sparse = csr_matrix(dtm)\n",
    "    dtm = dtm.astype(float)\n",
    "    lsa = TruncatedSVD(min(X_sparse.shape)-1, algorithm = 'arpack')\n",
    "    dtm_lsa = lsa.fit_transform(dtm)\n",
    "    dtm_lsa = Normalizer(copy=False).fit_transform(dtm_lsa)\n",
    "\n",
    "    df = pd.read_excel(path+\"\\input_file.xlsx\")\n",
    "    #example_des_b=df.reset_index()['Description'].values.tolist()\n",
    "    \n",
    "    #Label des Données brutes\n",
    "    df[col2] = df[col2][(~df[col2].duplicated()) | df[col2].isna()] \n",
    "    #df to list\n",
    "    example_des_b=df.reset_index()[col1].values.tolist()\n",
    "    example_des_b = [i for i in example_des_b if str(i) != 'nan']\n",
    "    example_m_b=df.reset_index()[col2].values.tolist()\n",
    "    example_m_b = list(dict.fromkeys(example_m_b))\n",
    "\n",
    "    \n",
    "    example_m_b = [i for i in example_m_b if str(i) != 'nan']\n",
    "    chunks_b= [example_des_b[x:x+chunk_size] for x in range(0, len(example_des_b), chunk_size)]\n",
    "    example_num_b=example_m_b+chunks_b[j]\n",
    "\n",
    "    \n",
    "    similarity = np.asarray(np.asmatrix(dtm_lsa) * np.asmatrix(dtm_lsa).T)\n",
    "    CrossValid=pd.DataFrame(similarity,index=example_num_b,columns=example_num_b)\n",
    "    Matrice_M_Phy=CrossValid.iloc[len(example_m_b):, 0:len(example_m_b)]\n",
    "    with open(path+\"\\outputt_file1.csv\", mode='a',newline='') as file:       \n",
    "        writer = csv.writer(file, delimiter=';', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        for index, row in Matrice_M_Phy.iterrows():  \n",
    "            writer.writerow([index,row.idxmax(),round(row.loc[row.idxmax()],4)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\\output{}.csv\".format(j)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
